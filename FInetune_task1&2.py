# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m9-zyFCS9CT7jZTVcy8skuyRx3PFmaBQ

Task 1 Zero-shot Baseline
"""

!pip -q install "pandas==2.2.2" "transformers>=4.42" "accelerate>=0.31" "bitsandbytes>=0.43.1" "peft>=0.11.1" "datasets>=2.20.0" "openpyxl"

"""1.导入库设置模型参数"""

import os, re
import pandas as pd
import numpy as np

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_IDS = [
    "Qwen/Qwen2.5-7B-Instruct"
]

MAX_NEW_TOKENS = 96
TEMPERATURE = 0.0
TOP_P = 0.95

LOAD_IN_4BIT = True
DEVICE_MAP = "auto"

DATA_XLSX = "train-test-data.xlsx"
OUT_DIR = "outputs_task1"
os.makedirs(OUT_DIR, exist_ok=True)

print("Torch:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())

"""2.导入数据"""

from pathlib import Path

if not Path(DATA_XLSX).exists():
    from google.colab import files
    print("Please upload 'train-test-data.xlsx'")
    uploaded = files.upload()
    print("Uploaded:", list(uploaded.keys()))

df = pd.read_excel(DATA_XLSX)
print("Columns:", df.columns.tolist())
print(df.head())

finding_col = "finding" if "finding" in df.columns else df.columns[0]
label_col   = "labels"  if "labels"  in df.columns else df.columns[1]

test_df = df[[finding_col, label_col]].dropna().reset_index(drop=True)
test_df.rename(columns={finding_col:"finding", label_col:"labels"}, inplace=True)

test_df["finding"] = test_df["finding"].astype(str)
test_df["labels"]  = test_df["labels"].fillna("").astype(str)

print("Test size:", len(test_df))
test_df.head(3)

"""3.设定Prompt模版"""

PROMPT_TEMPLATE = """You are a medical assistant. Extract only the disease names from the following radiology finding.
Output the disease names as a comma-separated list, with NO extra words, NO explanations, and NO duplicates.
If nothing is found, output: None

Radiology finding:
{finding}
"""
print(PROMPT_TEMPLATE)

"""4.加载模型"""

def load_model(model_id: str):
    print(f"Loading model: {model_id}")
    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    kw = dict(device_map=DEVICE_MAP)
    if LOAD_IN_4BIT:
        kw.update(dict(load_in_4bit=True))
    mdl = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, **kw)
    return tok, mdl

import math

@torch.no_grad()
def generate_labels(tokenizer, model, finding):
    if finding is None:
        finding_text = ""
    else:
        try:
            if isinstance(finding, float) and math.isnan(finding):
                finding_text = ""
            else:
                finding_text = str(finding)
        except Exception:
            finding_text = str(finding)

    prompt = PROMPT_TEMPLATE.format(finding=finding_text.strip())
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return text[len(prompt):].strip() if text.startswith(prompt) else text.strip()

"""5.解析"""

def normalize_label(s):
    return re.sub(r"[.;:]+$", "", s.strip().lower())

def parse_pred(s):
    if not s or s.strip().lower() in ["none", "nil"]:
        return set()
    parts = re.split(r",|;|\\n", s)
    return {normalize_label(p) for p in parts if normalize_label(p)}

def parse_gold(s):
    parts = re.split(r",|;|\\n", str(s))
    return {normalize_label(p) for p in parts if normalize_label(p)}

def prf1(pred, gold):
    tp = len(pred & gold); fp = len(pred - gold); fn = len(gold - pred)
    prec = tp/(tp+fp) if tp+fp else 0
    rec  = tp/(tp+fn) if tp+fn else 0
    f1   = 2*prec*rec/(prec+rec) if prec+rec else 0
    return prec, rec, f1

def jaccard(pred, gold):
    if not pred and not gold: return 1.0
    return len(pred & gold) / len(pred | gold)

"""6.执行和结果"""

all_summaries = []

for mid in MODEL_IDS:
    tokenizer, model = load_model(mid)
    rows = []
    for _, row in test_df.iterrows():
        finding, gold = row["finding"], parse_gold(row["labels"])
        pred_raw = generate_labels(tokenizer, model, finding)
        pred = parse_pred(pred_raw)
        p, r, f1 = prf1(pred, gold)
        j = jaccard(pred, gold)
        rows.append({
            "finding": finding,
            "gold": ", ".join(sorted(gold)),
            "pred_raw": pred_raw,
            "pred": ", ".join(sorted(pred)),
            "precision": p, "recall": r, "f1": f1, "jaccard": j
        })
    df_out = pd.DataFrame(rows)
    df_out.to_csv(f"{OUT_DIR}/pred_{mid.replace('/','_')}.csv", index=False)
    summary = df_out[["precision","recall","f1","jaccard"]].mean().to_dict()
    summary["model"] = mid
    all_summaries.append(summary)

pd.DataFrame(all_summaries)

"""
In this task, we established a **zero-shot baseline** using the Qwen Instruct model.  
The pipeline included:
1. Loading the pretrained Qwen model in a quantized setting.  
2. Designing prompts to extract disease names from radiology findings.  
3. Running inference on sample data.  
4. Parsing predictions and evaluating them against gold labels using Precision, Recall, and F1-score.  

Due to GPU and runtime limitations in Colab, only partial runs were demonstrated in this notebook.  
However, the full pipeline is complete and reproducible, and in a more powerful environment the model can be run across the entire dataset.  

This baseline provides a point of comparison for **Task 2**, where parameter-efficient fine-tuning (LoRA/QLoRA) will be applied to improve model performance."""



"""Task 2 Efficient Model Fine-tunning"""

from huggingface_hub import login
login("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")

import torch, subprocess
print("Torch:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
    try:
        print(subprocess.check_output(
            "nvidia-smi --query-gpu=name,memory.total,memory.used --format=csv,noheader",
            shell=True).decode())
    except: pass

!pip -q install -U transformers accelerate bitsandbytes peft trl datasets pandas openpyxl scikit-learn hf-transfer
import os
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
os.environ["BITSANDBYTES_NOWELCOME"] = "1"

from huggingface_hub import notebook_login
notebook_login()

"""Qwen3 4B with QLoRA + LoRA"""

import torch, re
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig
from trl import SFTConfig, SFTTrainer

MODEL_ID = "Qwen/Qwen3-4B-Instruct"
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    bnb_4bit_use_double_quant=True,
)

peft_cfg = LoraConfig(
    r=16, lora_alpha=32, lora_dropout=0.05,
    target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"],
    bias="none", task_type="CAUSAL_LM"
)

train_cfg = SFTConfig(
    output_dir="qwen3_lora_ckpt",
    num_train_epochs=2,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    logging_steps=20,
    save_strategy="steps",
    save_steps=100,
    bf16=torch.cuda.is_available(),
    optim="paged_adamw_32bit",
    gradient_checkpointing=True,
    eval_strategy="steps",
    eval_steps=100,
    report_to="none",
)

SYSTEM_PROMPT = (
    "You are a medical assistant. Extract only the disease names from the following radiology finding. "
    "Return a comma-separated list with NO extra words."
)

"""Load data"""

import pandas as pd
from sklearn.model_selection import train_test_split

DATA_XLSX = "train-test-data.xlsx"
df = pd.read_excel(DATA_XLSX)

assert {"input_finding","output_disease"}.issubset(df.columns)
df = df[["input_finding","output_disease"]].dropna().reset_index(drop=True)

def normalize_labels(s: str) -> str:
    s = str(s)
    s = re.sub(r"[，；;|/]+", ",", s)
    s = re.sub(r"\s*,\s*", ",", s)
    s = re.sub(r",+", ",", s).strip(" ,.;:")
    return s

df["input_finding"]  = df["input_finding"].astype(str).str.strip()
df["output_disease"] = df["output_disease"].apply(normalize_labels)

train_df, val_df = train_test_split(df, test_size=0.10, random_state=42, shuffle=True)

def to_text(row):
    finding = row["input_finding"]
    labels  = row["output_disease"] or "None"
    prompt  = f"{SYSTEM_PROMPT}\n\nRadiology finding:\n{finding}\n\nAnswer:"
    return {"text": f"{prompt} {labels}"}

train_data = train_df.apply(to_text, axis=1).tolist()
val_data   = val_df.apply(to_text,   axis=1).tolist()

len(train_data), len(val_data), train_data[0]["text"][:240]

from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_ID = "Qwen/Qwen2.5-7B-Instruct"

tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)

base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_cfg,
    device_map="auto",
    low_cpu_mem_usage=True
)

print("Loaded to:", base_model.device)

"""Trainer"""

from inspect import signature
from trl import SFTConfig, SFTTrainer
import torch

assert isinstance(train_data, (list, tuple)) and len(train_data) > 0, "train_data must be a non-empty list"
assert isinstance(val_data, (list, tuple)) and len(val_data) > 0, "val_data must be a non-empty list"
if isinstance(train_data[0], dict):
    assert "text" in train_data[0], "Each training sample must have a 'text' field"
else:
    train_data = [{"text": s} for s in train_data]
    val_data   = [{"text": s} for s in val_data]

cfg_params = signature(SFTConfig.__init__).parameters
cfg_kwargs = dict(
    output_dir="qwen3_lora_ckpt",
    num_train_epochs=2,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,
    logging_steps=20,
    save_strategy="steps",
    save_steps=100,
    bf16=torch.cuda.is_available(),
    optim="paged_adamw_32bit",
    gradient_checkpointing=True,
    eval_strategy="steps",
    eval_steps=100,
    report_to="none",
)

"""Inference with the fine-tuned adapter

"""

from peft import PeftModel

ft_base = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    quantization_config=bnb_cfg,
    device_map="auto",
    low_cpu_mem_usage=True
)
ft_model = PeftModel.from_pretrained(ft_base, train_cfg.output_dir)
ft_model.eval()

def generate(ft_model, tok, finding, max_new_tokens=96, temperature=0.0, top_p=0.95):
    prompt = f"{SYSTEM_PROMPT}\n\nRadiology finding:\n{finding}\n\nAnswer:"
    inputs = tok(prompt, return_tensors="pt").to(ft_model.device)
    out = ft_model.generate(
        **inputs, max_new_tokens=max_new_tokens,
        do_sample=(temperature>0), temperature=temperature, top_p=top_p
    )
    text = tok.decode(out[0], skip_special_tokens=True)
    return text.split("Answer:")[-1].strip()

print(generate(ft_model, tok, "The liver surface is irregular with multiple small nodules."))

"""Evaluation"""

import numpy as np

def norm_label(s):
    return re.sub(r"[.;:]+$", "", s.strip().lower())

def to_set(s):
    parts = re.split(r",|;|\n", str(s))
    return {norm_label(p) for p in parts if norm_label(p)}

def prf1(pred, gold):
    tp = len(pred & gold); fp = len(pred - gold); fn = len(gold - pred)
    p = tp/(tp+fp) if tp+fp else 0
    r = tp/(tp+fn) if tp+fn else 0
    f = 2*p*r/(p+r) if p+r else 0
    return p,r,f

rows, P,R,F,J = [], [], [], [], []
for _, row in val_df.iterrows():
    finding, gold = row["input_finding"], to_set(row["output_disease"])
    pred_raw = generate(ft_model, tok, finding)
    pred     = to_set(pred_raw)
    p,r,f = prf1(pred, gold)
    j = (len(pred & gold) / len(pred | gold)) if (pred or gold) else 1.0
    rows.append({
        "finding": finding,
        "gold": ", ".join(sorted(gold)),
        "pred_raw": pred_raw,
        "pred": ", ".join(sorted(pred)),
        "precision": p, "recall": r, "f1": f, "jaccard": j
    })
    P.append(p); R.append(r); F.append(f); J.append(j)

eval_df = pd.DataFrame(rows)
print(eval_df.head(3))
print("== Validation (mean per-sample) ==")
print("Precision:", round(np.mean(P),4),
      "Recall:",    round(np.mean(R),4),
      "F1:",        round(np.mean(F),4),
      "Jaccard:",   round(np.mean(J),4))

eval_df.to_csv("eval_ft_lora_val.csv", index=False)